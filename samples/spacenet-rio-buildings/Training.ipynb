{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# mapping-challenge-mask_rcnn-training\n",
    "![CrowdAI-Logo](https://github.com/crowdAI/crowdai/raw/master/app/assets/images/misc/crowdai-logo-smile.svg?sanitize=true)\n",
    "\n",
    "This notebook contains the baseline code for the training a vanilla [Mask RCNN](https://arxiv.org/abs/1703.06870) model for the [crowdAI Mapping Challenge](https://www.crowdai.org/challenges/mapping-challenge).\n",
    "\n",
    "This code is adapted from the [Mask RCNN]() tensorflow implementation available here : [https://github.com/matterport/Mask_RCNN](https://github.com/matterport/Mask_RCNN).\n",
    "\n",
    "First we begin by importing all the necessary dependencies : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "# Download and install the Python COCO tools from https://github.com/waleedka/coco\n",
    "# That's a fork from the original https://github.com/pdollar/coco with a bug\n",
    "# fix for Python 3.\n",
    "# I submitted a pull request https://github.com/cocodataset/cocoapi/pull/50\n",
    "# If the PR is merged then use the original repo.\n",
    "# Note: Edit PythonAPI/Makefile and replace \"python\" with \"python3\".\n",
    "#  \n",
    "# A quick one liner to install the library \n",
    "# !pip install git+https://github.com/waleedka/coco.git#subdirectory=PythonAPI\n",
    "\n",
    "from pycocotools.coco import COCO\n",
    "from pycocotools.cocoeval import COCOeval\n",
    "from pycocotools import mask as maskUtils\n",
    "\n",
    "from evaluate import build_coco_results, evaluate_coco\n",
    "from dataset import SpaceNetChallengeDataset\n",
    "\n",
    "import zipfile\n",
    "import urllib.request\n",
    "import shutil\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset location \n",
    "Now we have to download all the files in the datasets section and untar them to have the following structure :\n",
    "```\n",
    "├── data\n",
    "|   ├── pretrained_weights.h5 (already included in this repository)\n",
    "│   ├── test\n",
    "│   │   └── images/\n",
    "│   │   └── annotation.json\n",
    "│   ├── train\n",
    "│   │   └── images/\n",
    "│   │   └── annotation.json\n",
    "│   └── val\n",
    "│       └── images/\n",
    "│       └── annotation.json\n",
    "```\n",
    "Note that the `pretrained_weights.h5` (available at [https://www.crowdai.org/challenges/mapping-challenge/dataset_files](https://www.crowdai.org/challenges/mapping-challenge/dataset_files)) are the weights used for the baseline submission, and are obtained by running the learning schedule mentioned later in the experiment. In the said experiment, the initial weights used can be found [here](https://github.com/matterport/Mask_RCNN/releases/download/v2.1/mask_rcnn_balloon.h5). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Root directory of the project\n",
    "ROOT_DIR = os.path.abspath(\"../../\")\n",
    "\n",
    "# Import Mask RCNN\n",
    "sys.path.append(ROOT_DIR)  # To find local version of the library\n",
    "from mrcnn.config import Config\n",
    "from mrcnn import model as modellib, utils\n",
    "\n",
    "\n",
    "PRETRAINED_MODEL_PATH = os.path.join(ROOT_DIR,\"data/\" \"pretrained_weights.h5\")\n",
    "LOGS_DIRECTORY = os.path.join(ROOT_DIR, \"logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "BACKBONE                       resnet101\n",
      "BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n",
      "BATCH_SIZE                     6\n",
      "BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n",
      "COMPUTE_BACKBONE_SHAPE         None\n",
      "DETECTION_MAX_INSTANCES        400\n",
      "DETECTION_MIN_CONFIDENCE       0.7\n",
      "DETECTION_NMS_THRESHOLD        0.3\n",
      "FPN_CLASSIF_FC_LAYERS_SIZE     1024\n",
      "GPU_COUNT                      1\n",
      "GRADIENT_CLIP_NORM             5.0\n",
      "IMAGES_PER_GPU                 6\n",
      "IMAGE_CHANNEL_COUNT            3\n",
      "IMAGE_MAX_DIM                  320\n",
      "IMAGE_META_SIZE                14\n",
      "IMAGE_MIN_DIM                  320\n",
      "IMAGE_MIN_SCALE                0\n",
      "IMAGE_RESIZE_MODE              crop\n",
      "IMAGE_SHAPE                    [320 320   3]\n",
      "LEARNING_MOMENTUM              0.9\n",
      "LEARNING_RATE                  0.001\n",
      "LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n",
      "MASK_POOL_SIZE                 14\n",
      "MASK_SHAPE                     [28, 28]\n",
      "MAX_GT_INSTANCES               100\n",
      "MEAN_PIXEL                     [81.0541178  86.41590797 64.59318455]\n",
      "MINI_MASK_SHAPE                (56, 56)\n",
      "NAME                           spacenet-rio-buildings\n",
      "NUM_CLASSES                    2\n",
      "POOL_SIZE                      7\n",
      "POST_NMS_ROIS_INFERENCE        1000\n",
      "POST_NMS_ROIS_TRAINING         2000\n",
      "PRE_NMS_LIMIT                  6000\n",
      "ROI_POSITIVE_RATIO             0.33\n",
      "RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n",
      "RPN_ANCHOR_SCALES              (8, 16, 32, 64, 128)\n",
      "RPN_ANCHOR_STRIDE              1\n",
      "RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n",
      "RPN_NMS_THRESHOLD              0.7\n",
      "RPN_TRAIN_ANCHORS_PER_IMAGE    256\n",
      "STEPS_PER_EPOCH                1000\n",
      "TOP_DOWN_PYRAMID_SIZE          256\n",
      "TRAIN_BN                       False\n",
      "TRAIN_ROIS_PER_IMAGE           200\n",
      "USE_MINI_MASK                  True\n",
      "USE_RPN_ROIS                   True\n",
      "VALIDATION_STEPS               50\n",
      "WEIGHT_DECAY                   0.0001\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from dataset import SpaceNetChallengeConfig\n",
    "config = SpaceNetChallengeConfig()\n",
    "config.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = modellib.MaskRCNN(mode=\"training\", config=config, model_dir=LOGS_DIRECTORY)\n",
    "# Load pretrained weights\n",
    "model_path = PRETRAINED_MODEL_PATH\n",
    "model.load_weights(model_path, by_name=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Training and Validation Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Annotation Path  ../../data/processedBuildingLabels/vectordata/summarydata/AOI_1_RIO_polygons_solution_3band.csv\n",
      "Image Dir  ../../data/processedBuildingLabels/3band\n",
      "RGB mean: [81.0541178  86.41590797 64.59318455]\n",
      "Building Counts:\n",
      "count    3879.000000\n",
      "mean       59.181232\n",
      "std        60.581133\n",
      "min         1.000000\n",
      "25%         7.000000\n",
      "50%        35.000000\n",
      "75%       103.000000\n",
      "max       305.000000\n",
      "dtype: float64\n",
      "Building Widths (m):\n",
      "count    229564.000000\n",
      "mean         12.621780\n",
      "std           7.121238\n",
      "min           0.000345\n",
      "25%           8.642687\n",
      "50%          11.799508\n",
      "75%          15.422596\n",
      "max         219.500003\n",
      "dtype: float64\n",
      "Building Heights (m):\n",
      "count    229564.000000\n",
      "mean         11.937064\n",
      "std           6.730427\n",
      "min           0.000764\n",
      "25%           8.165370\n",
      "50%          11.195470\n",
      "75%          14.680316\n",
      "max         203.000005\n",
      "dtype: float64\n",
      "Annotation Path  ../../data/processedBuildingLabels/vectordata/summarydata/AOI_1_RIO_polygons_solution_3band.csv\n",
      "Image Dir  ../../data/processedBuildingLabels/3band\n",
      "RGB mean: [80.95048491 86.84746543 64.79199872]\n",
      "Building Counts:\n",
      "count    431.000000\n",
      "mean      54.958237\n",
      "std       58.201399\n",
      "min        1.000000\n",
      "25%        5.000000\n",
      "50%       28.000000\n",
      "75%       99.000000\n",
      "max      287.000000\n",
      "dtype: float64\n",
      "Building Widths (m):\n",
      "count    23687.000000\n",
      "mean        12.655945\n",
      "std          7.545398\n",
      "min          0.000214\n",
      "25%          8.584236\n",
      "50%         11.730437\n",
      "75%         15.424999\n",
      "max        193.108449\n",
      "dtype: float64\n",
      "Building Heights (m):\n",
      "count    23687.000000\n",
      "mean        12.015106\n",
      "std          7.358918\n",
      "min          0.001087\n",
      "25%          8.114397\n",
      "50%         11.084481\n",
      "75%         14.671867\n",
      "max        203.000002\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Load training dataset\n",
    "dataset_train = SpaceNetChallengeDataset()\n",
    "dataset_train.load_dataset(dataset_dir=\"../../data\", subset=\"train\")\n",
    "dataset_train.prepare()\n",
    "\n",
    "# Load validation dataset\n",
    "dataset_val = SpaceNetChallengeDataset()\n",
    "val_coco = dataset_val.load_dataset(dataset_dir=\"../../data\", subset=\"val\")\n",
    "dataset_val.prepare()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training network heads\n",
      "\n",
      "Starting at epoch 0. LR=0.001\n",
      "\n",
      "Checkpoint Path: /Users/yoninachmany/Documents/DymaxionLabs/crowdai-mapping-challenge-mask-rcnn/logs/spacenet-rio-buildings20181207T1703/mask_rcnn_spacenet-rio-buildings_{epoch:04d}.h5\n",
      "Selecting layers to train\n",
      "fpn_c5p5               (Conv2D)\n",
      "fpn_c4p4               (Conv2D)\n",
      "fpn_c3p3               (Conv2D)\n",
      "fpn_c2p2               (Conv2D)\n",
      "fpn_p5                 (Conv2D)\n",
      "fpn_p2                 (Conv2D)\n",
      "fpn_p3                 (Conv2D)\n",
      "fpn_p4                 (Conv2D)\n",
      "In model:  rpn_model\n",
      "    rpn_conv_shared        (Conv2D)\n",
      "    rpn_class_raw          (Conv2D)\n",
      "    rpn_bbox_pred          (Conv2D)\n",
      "mrcnn_mask_conv1       (TimeDistributed)\n",
      "mrcnn_mask_bn1         (TimeDistributed)\n",
      "mrcnn_mask_conv2       (TimeDistributed)\n",
      "mrcnn_mask_bn2         (TimeDistributed)\n",
      "mrcnn_class_conv1      (TimeDistributed)\n",
      "mrcnn_class_bn1        (TimeDistributed)\n",
      "mrcnn_mask_conv3       (TimeDistributed)\n",
      "mrcnn_mask_bn3         (TimeDistributed)\n",
      "mrcnn_class_conv2      (TimeDistributed)\n",
      "mrcnn_class_bn2        (TimeDistributed)\n",
      "mrcnn_mask_conv4       (TimeDistributed)\n",
      "mrcnn_mask_bn4         (TimeDistributed)\n",
      "mrcnn_bbox_fc          (TimeDistributed)\n",
      "mrcnn_mask_deconv      (TimeDistributed)\n",
      "mrcnn_class_logits     (TimeDistributed)\n",
      "mrcnn_mask             (TimeDistributed)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/yoninachmany/Documents/DymaxionLabs/crowdai-mapping-challenge-mask-rcnn/venv/lib/python3.6/site-packages/tensorflow/python/ops/gradients_impl.py:112: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n",
      "/Users/yoninachmany/Documents/DymaxionLabs/crowdai-mapping-challenge-mask-rcnn/venv/lib/python3.6/site-packages/keras/engine/training_generator.py:47: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n",
      "  UserWarning('Using a generator with `use_multiprocessing=True`'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method BaseGeometry.__del__ of <shapely.geometry.polygon.LinearRing object at 0x17cf3be80>>\n",
      "Process ForkPoolWorker-5:\n",
      "Process ForkPoolWorker-3:\n",
      "Process ForkPoolWorker-7:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yoninachmany/Documents/DymaxionLabs/crowdai-mapping-challenge-mask-rcnn/venv/lib/python3.6/site-packages/shapely/geometry/base.py\", line 234, in __del__\n",
      "    self.empty(val=None)\n",
      "  File \"/Users/yoninachmany/Documents/DymaxionLabs/crowdai-mapping-challenge-mask-rcnn/venv/lib/python3.6/site-packages/shapely/geometry/base.py\", line 222, in empty\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/Users/yoninachmany/Documents/DymaxionLabs/crowdai-mapping-challenge-mask-rcnn/venv/lib/python3.6/site-packages/keras/utils/data_utils.py\", line 626, in next_sample\n",
      "    return six.next(_SHARED_SEQUENCES[uid])\n",
      "  File \"/Users/yoninachmany/Documents/DymaxionLabs/crowdai-mapping-challenge-mask-rcnn/venv/lib/python3.6/site-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py\", line 1710, in data_generator\n",
      "    use_mini_mask=config.USE_MINI_MASK)\n",
      "    def empty(self, val=EMPTY):\n",
      "  File \"/Users/yoninachmany/Documents/DymaxionLabs/crowdai-mapping-challenge-mask-rcnn/venv/lib/python3.6/site-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py\", line 1213, in load_image_gt\n",
      "    mask, class_ids = dataset.load_mask(image_id)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/yoninachmany/Documents/DymaxionLabs/crowdai-mapping-challenge-mask-rcnn/samples/spacenet-rio-buildings/dataset.py\", line 158, in load_mask\n",
      "    points = np.vstack((x, y)).T\n",
      "KeyboardInterrupt\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/yoninachmany/Documents/DymaxionLabs/crowdai-mapping-challenge-mask-rcnn/venv/lib/python3.6/site-packages/numpy/core/shape_base.py\", line 234, in vstack\n",
      "    return _nx.concatenate([atleast_2d(_m) for _m in tup], 0)\n",
      "KeyboardInterrupt\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/Users/yoninachmany/Documents/DymaxionLabs/crowdai-mapping-challenge-mask-rcnn/venv/lib/python3.6/site-packages/keras/utils/data_utils.py\", line 626, in next_sample\n",
      "    return six.next(_SHARED_SEQUENCES[uid])\n",
      "Process ForkPoolWorker-2:\n",
      "Process ForkPoolWorker-6:\n",
      "  File \"/Users/yoninachmany/Documents/DymaxionLabs/crowdai-mapping-challenge-mask-rcnn/venv/lib/python3.6/site-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py\", line 1710, in data_generator\n",
      "    use_mini_mask=config.USE_MINI_MASK)\n",
      "Traceback (most recent call last):\n",
      "Process ForkPoolWorker-8:\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/yoninachmany/Documents/DymaxionLabs/crowdai-mapping-challenge-mask-rcnn/venv/lib/python3.6/site-packages/keras/utils/data_utils.py\", line 626, in next_sample\n",
      "    return six.next(_SHARED_SEQUENCES[uid])\n",
      "  File \"/Users/yoninachmany/Documents/DymaxionLabs/crowdai-mapping-challenge-mask-rcnn/venv/lib/python3.6/site-packages/keras/utils/data_utils.py\", line 626, in next_sample\n",
      "    return six.next(_SHARED_SEQUENCES[uid])\n",
      "  File \"/Users/yoninachmany/Documents/DymaxionLabs/crowdai-mapping-challenge-mask-rcnn/venv/lib/python3.6/site-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py\", line 1710, in data_generator\n",
      "    use_mini_mask=config.USE_MINI_MASK)\n",
      "  File \"/Users/yoninachmany/Documents/DymaxionLabs/crowdai-mapping-challenge-mask-rcnn/venv/lib/python3.6/site-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py\", line 1710, in data_generator\n",
      "    use_mini_mask=config.USE_MINI_MASK)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/yoninachmany/Documents/DymaxionLabs/crowdai-mapping-challenge-mask-rcnn/venv/lib/python3.6/site-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py\", line 1255, in load_image_gt\n",
      "    hooks=imgaug.HooksImages(activator=hook))\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/Users/yoninachmany/Documents/DymaxionLabs/crowdai-mapping-challenge-mask-rcnn/venv/lib/python3.6/site-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py\", line 1281, in load_image_gt\n",
      "    mask = utils.minimize_mask(bbox, mask, config.MINI_MASK_SHAPE)\n",
      "  File \"/Users/yoninachmany/Documents/DymaxionLabs/crowdai-mapping-challenge-mask-rcnn/venv/lib/python3.6/site-packages/imgaug/augmenters/meta.py\", line 323, in augment_image\n",
      "    return self.augment_images([image], hooks=hooks)[0]\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/yoninachmany/Documents/DymaxionLabs/crowdai-mapping-challenge-mask-rcnn/venv/lib/python3.6/site-packages/imgaug/augmenters/meta.py\", line 431, in augment_images\n",
      "    hooks=hooks\n",
      "  File \"/Users/yoninachmany/Documents/DymaxionLabs/crowdai-mapping-challenge-mask-rcnn/venv/lib/python3.6/site-packages/keras/utils/data_utils.py\", line 626, in next_sample\n",
      "    return six.next(_SHARED_SEQUENCES[uid])\n",
      "  File \"/Users/yoninachmany/Documents/DymaxionLabs/crowdai-mapping-challenge-mask-rcnn/venv/lib/python3.6/site-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/utils.py\", line 530, in minimize_mask\n",
      "    m = resize(m, mini_shape)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/yoninachmany/Documents/DymaxionLabs/crowdai-mapping-challenge-mask-rcnn/venv/lib/python3.6/site-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py\", line 1213, in load_image_gt\n",
      "    mask, class_ids = dataset.load_mask(image_id)\n",
      "  File \"/Users/yoninachmany/Documents/DymaxionLabs/crowdai-mapping-challenge-mask-rcnn/venv/lib/python3.6/site-packages/imgaug/augmenters/meta.py\", line 1522, in _augment_images\n",
      "    hooks=hooks\n",
      "  File \"/Users/yoninachmany/Documents/DymaxionLabs/crowdai-mapping-challenge-mask-rcnn/venv/lib/python3.6/site-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py\", line 1710, in data_generator\n",
      "    use_mini_mask=config.USE_MINI_MASK)\n",
      "  File \"/Users/yoninachmany/Documents/DymaxionLabs/crowdai-mapping-challenge-mask-rcnn/venv/lib/python3.6/site-packages/imgaug/augmenters/meta.py\", line 402, in augment_images\n",
      "    image_copy = np.copy(image)\n",
      "  File \"/Users/yoninachmany/Documents/DymaxionLabs/crowdai-mapping-challenge-mask-rcnn/venv/lib/python3.6/site-packages/numpy/lib/function_base.py\", line 733, in copy\n",
      "    return array(a, order=order, copy=True)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/yoninachmany/Documents/DymaxionLabs/crowdai-mapping-challenge-mask-rcnn/venv/lib/python3.6/site-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/utils.py\", line 901, in resize\n",
      "    anti_aliasing_sigma=anti_aliasing_sigma)\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/yoninachmany/Documents/DymaxionLabs/crowdai-mapping-challenge-mask-rcnn/venv/lib/python3.6/site-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py\", line 1265, in load_image_gt\n",
      "    mask = mask[:, :, _idx]\n",
      "  File \"/Users/yoninachmany/Documents/DymaxionLabs/crowdai-mapping-challenge-mask-rcnn/venv/lib/python3.6/site-packages/skimage/transform/_warps.py\", line 165, in resize\n",
      "    tform.estimate(src_corners, dst_corners)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/Users/yoninachmany/Documents/DymaxionLabs/crowdai-mapping-challenge-mask-rcnn/venv/lib/python3.6/site-packages/skimage/transform/_geometric.py\", line 688, in estimate\n",
      "    H = np.dot(np.linalg.inv(dst_matrix), np.dot(H, src_matrix))\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/yoninachmany/Documents/DymaxionLabs/crowdai-mapping-challenge-mask-rcnn/venv/lib/python3.6/site-packages/keras/utils/data_utils.py\", line 626, in next_sample\n",
      "    return six.next(_SHARED_SEQUENCES[uid])\n",
      "  File \"/Users/yoninachmany/Documents/DymaxionLabs/crowdai-mapping-challenge-mask-rcnn/venv/lib/python3.6/site-packages/numpy/linalg/linalg.py\", line 532, in inv\n",
      "    ainv = _umath_linalg.inv(a, signature=signature, extobj=extobj)\n",
      "  File \"/Users/yoninachmany/Documents/DymaxionLabs/crowdai-mapping-challenge-mask-rcnn/venv/lib/python3.6/site-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py\", line 1710, in data_generator\n",
      "    use_mini_mask=config.USE_MINI_MASK)\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/yoninachmany/Documents/DymaxionLabs/crowdai-mapping-challenge-mask-rcnn/samples/spacenet-rio-buildings/dataset.py\", line 161, in load_mask\n",
      "    grid = path.contains_points(points)\n",
      "  File \"/Users/yoninachmany/Documents/DymaxionLabs/crowdai-mapping-challenge-mask-rcnn/venv/lib/python3.6/site-packages/matplotlib/path.py\", line 502, in contains_points\n",
      "    result = _path.points_in_path(points, radius, self, transform)\n",
      "KeyboardInterrupt\n",
      "  File \"/Users/yoninachmany/Documents/DymaxionLabs/crowdai-mapping-challenge-mask-rcnn/venv/lib/python3.6/site-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py\", line 1281, in load_image_gt\n",
      "    mask = utils.minimize_mask(bbox, mask, config.MINI_MASK_SHAPE)\n",
      "  File \"/Users/yoninachmany/Documents/DymaxionLabs/crowdai-mapping-challenge-mask-rcnn/venv/lib/python3.6/site-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/utils.py\", line 530, in minimize_mask\n",
      "    m = resize(m, mini_shape)\n",
      "  File \"/Users/yoninachmany/Documents/DymaxionLabs/crowdai-mapping-challenge-mask-rcnn/venv/lib/python3.6/site-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/utils.py\", line 901, in resize\n",
      "    anti_aliasing_sigma=anti_aliasing_sigma)\n",
      "  File \"/Users/yoninachmany/Documents/DymaxionLabs/crowdai-mapping-challenge-mask-rcnn/venv/lib/python3.6/site-packages/skimage/transform/_warps.py\", line 165, in resize\n",
      "    tform.estimate(src_corners, dst_corners)\n",
      "  File \"/Users/yoninachmany/Documents/DymaxionLabs/crowdai-mapping-challenge-mask-rcnn/venv/lib/python3.6/site-packages/skimage/transform/_geometric.py\", line 688, in estimate\n",
      "    H = np.dot(np.linalg.inv(dst_matrix), np.dot(H, src_matrix))\n",
      "  File \"/Users/yoninachmany/Documents/DymaxionLabs/crowdai-mapping-challenge-mask-rcnn/venv/lib/python3.6/site-packages/numpy/linalg/linalg.py\", line 532, in inv\n",
      "    ainv = _umath_linalg.inv(a, signature=signature, extobj=extobj)\n",
      "KeyboardInterrupt\n",
      "Process ForkPoolWorker-1:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.6/lib/python3.6/multiprocessing/pool.py\", line 119, in worker\n",
      "    result = (True, func(*args, **kwds))\n",
      "  File \"/Users/yoninachmany/Documents/DymaxionLabs/crowdai-mapping-challenge-mask-rcnn/venv/lib/python3.6/site-packages/keras/utils/data_utils.py\", line 626, in next_sample\n",
      "    return six.next(_SHARED_SEQUENCES[uid])\n",
      "  File \"/Users/yoninachmany/Documents/DymaxionLabs/crowdai-mapping-challenge-mask-rcnn/venv/lib/python3.6/site-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py\", line 1710, in data_generator\n",
      "    use_mini_mask=config.USE_MINI_MASK)\n",
      "  File \"/Users/yoninachmany/Documents/DymaxionLabs/crowdai-mapping-challenge-mask-rcnn/venv/lib/python3.6/site-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/model.py\", line 1221, in load_image_gt\n",
      "    mask = utils.resize_mask(mask, scale, padding, crop)\n",
      "  File \"/Users/yoninachmany/Documents/DymaxionLabs/crowdai-mapping-challenge-mask-rcnn/venv/lib/python3.6/site-packages/mask_rcnn-2.1-py3.6.egg/mrcnn/utils.py\", line 506, in resize_mask\n",
      "    mask = scipy.ndimage.zoom(mask, zoom=[scale, scale, 1], order=0)\n",
      "  File \"/Users/yoninachmany/Documents/DymaxionLabs/crowdai-mapping-challenge-mask-rcnn/venv/lib/python3.6/site-packages/scipy/ndimage/interpolation.py\", line 595, in zoom\n",
      "    _nd_image.zoom_shift(filtered, zoom, None, output, order, mode, cval)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "# *** This training schedule is an example. Update to your needs ***\n",
    "from imgaug import augmenters as iaa\n",
    "from imgaug import parameters as iap\n",
    "\n",
    "# Inspired by SIMDRWN/YOLT: https://github.com/CosmiQ/simrdwn/blob/master/core/yolt_data_prep_funcs.py#L1003-L1182\n",
    "augmentation = iaa.Sequential([\n",
    "    iaa.WithColorspace(to_colorspace=\"HSV\", from_colorspace=\"RGB\", children=[\n",
    "        iaa.WithChannels([0,1], iaa.Multiply((0.5, 1.5))),\n",
    "        iaa.WithChannels(2, iaa.Multiply((0.7, 1.3)))\n",
    "    ]),\n",
    "    iaa.OneOf([\n",
    "        iaa.Flipud(1),\n",
    "        iaa.Fliplr(1),\n",
    "        iaa.Affine(rotate=iap.Uniform(0, 90)),\n",
    "        iaa.Affine(rotate=90),\n",
    "        iaa.Affine(rotate=iap.Uniform(90, 180)),\n",
    "        iaa.Affine(rotate=180),\n",
    "        iaa.Affine(rotate=iap.Uniform(180, 270)),\n",
    "        iaa.Affine(rotate=270),\n",
    "        iaa.Affine(rotate=iap.Uniform(270, 360)),\n",
    "        iaa.Affine(rotate=360),\n",
    "    ])\n",
    "])\n",
    "\n",
    "# Training - Stage 1\n",
    "print(\"Training network heads\")\n",
    "model.train(dataset_train, dataset_val,\n",
    "            learning_rate=config.LEARNING_RATE,\n",
    "            epochs=40,\n",
    "            layers='heads',\n",
    "            augmentation=augmentation)\n",
    "\n",
    "# Training - Stage 2\n",
    "# Finetune layers from ResNet stage 4 and up\n",
    "print(\"Fine tune Resnet stage 4 and up\")\n",
    "model.train(dataset_train, dataset_val,\n",
    "            learning_rate=config.LEARNING_RATE,\n",
    "            epochs=120,\n",
    "            layers='4+',\n",
    "            augmentation=augmentation)\n",
    "\n",
    "# Training - Stage 3\n",
    "# Fine tune all layers\n",
    "print(\"Fine tune all layers\")\n",
    "model.train(dataset_train, dataset_val,\n",
    "            learning_rate=config.LEARNING_RATE / 10,\n",
    "            epochs=160,\n",
    "            layers='all',\n",
    "            augmentation=augmentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you can monitor the training by running : \n",
    "```\n",
    "tensorboard --logdir=logs/[path-to-your-experiment-logdir]\n",
    "```\n",
    "and if everything works great, you should see something like : \n",
    "![loss-plot](../../images/loss-plot.png)\n",
    "\n",
    "# Author\n",
    "Sharada Mohanty [sharada.mohanty@epfl.ch](sharada.mohanty@epfl.ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
